"""CEO Agent â€” top-level dispatcher for J.A.R.V.I.S.

Responsibilities:
- Parse user intent and dispatch to appropriate workers
- Emotion detection â†’ empathetic response path
- Inject SOUL.md persona into all interactions
- Skill invocation via SkillRegistry (Task 8.3)
- Proactive web search: detect need â†’ fetch â†’ inject into context
- Reactive tool-use: LLM can invoke [FETCH:url] / [SEARCH:query] as fallback
- Memory integration for context continuity
"""

from __future__ import annotations

import re
import time
from typing import Any
from urllib.parse import quote_plus

from loguru import logger

from clients.base_client import ChatMessage, ChatResponse
from core.model_router import ModelRole, ModelRouter, RouterError
from core.react_executor import ReactExecutor, FuseState, TaskResult
from core.security_gate import OperationType, OperationVerdict

# Pattern for LLM tool calls in response text (fallback)
_TOOL_PATTERN = re.compile(r'\[(?:FETCH|SEARCH):([^\]]+)\]')

# â”€â”€ Proactive web search detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Patterns that indicate user needs web information
_WEB_NEED_PATTERNS = re.compile(
    r"å¹«æˆ‘æŸ¥|å¹«æˆ‘æœ|å¹«æˆ‘æ‰¾|æŸ¥ä¸€ä¸‹|æœä¸€ä¸‹|æœå°‹|æœç´¢|æŸ¥è©¢|"
    r"ä¸Šç¶².*?(?:æŸ¥|çœ‹|æœ|æ‰¾)|é€£å¤–ç¶²|é€£ç¶²è·¯|"
    r"(?:ä»Šå¤©|ä»Šæ—¥|ç¾åœ¨|ç›®å‰|æœ€æ–°|æœ€è¿‘).*?(?:å¤©æ°£|æ–°è|æ¶ˆæ¯|è¡Œæƒ…|åƒ¹æ ¼|å ±å°)|"
    r"(?:å¤©æ°£|æ–°è|è¡Œæƒ…).*?(?:æ€[éº¼æ¨£]|å¦‚ä½•|å¤šå°‘|ä»€éº¼)|"
    r"(?:è‚¡åƒ¹|åŒ¯ç‡|æ¯”ç‰¹å¹£|bitcoin|btc|eth|åŠ å¯†è²¨å¹£).*?(?:å¤šå°‘|ç¾åœ¨|ä»Šå¤©|å¹¾|æ¼²|è·Œ)?|"
    r"å¤šå°‘éŒ¢|å“ªè£¡è²·|æ€éº¼å»|å¹¾é».*?(?:é–‹|é—œ|ç‡Ÿæ¥­)|"
    r"https?://\S+",
    re.IGNORECASE,
)

# Extract URL from user message for direct fetch
_URL_IN_MSG = re.compile(r'(https?://\S+)')

# Prefixes to strip when extracting search query
_SEARCH_PREFIX = re.compile(
    r"^(?:å¹«æˆ‘|è«‹ä½ ?|éº»ç…©)?(?:æŸ¥ä¸€ä¸‹|æœä¸€ä¸‹|æœå°‹|æœç´¢|æŸ¥è©¢|æŸ¥|æœ|æ‰¾|çœ‹ä¸€ä¸‹|çœ‹çœ‹)\s*",
)


class CEOAgent:
    """Central orchestrator â€” all user interactions flow through here.

    Usage:
        ceo = CEOAgent(
            model_router=router,
            soul=soul,
            emotion_classifier=emotion,
            memos=memos,
            skill_registry=registry,
            security_gate=security,
        )
        response = await ceo.handle_message("å¹«æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©è¡Œç¨‹")
    """

    def __init__(
        self,
        model_router: ModelRouter,
        soul: Any = None,
        emotion_classifier: Any = None,
        memos: Any = None,
        skill_registry: Any = None,
        security_gate: Any = None,
        workers: dict[str, Any] | None = None,
        markdown_memory: Any = None,
    ):
        self.router = model_router
        self.soul = soul
        self.emotion = emotion_classifier
        self.memos = memos
        self.skills = skill_registry
        self.security = security_gate
        self.workers = workers or {}
        self.md_memory = markdown_memory
        self.memory_search: Any = None  # G6: set externally
        self.pending: Any = None  # H4: PendingTaskManager, set externally
        self._react: ReactExecutor | None = None
        self._fuse = FuseState()
        self._persona = "jarvis"
        self._session_id = "default"
        self._last_skill_failure: str | None = None
        self._silent_until: float = 0.0  # Patch D: humanized silent mode
        # G4: Session transcript tracking
        self._session_transcript: list[tuple[str, str, str]] = []  # (role, persona, text)
        self._last_message_time: float = 0.0
        self._session_idle_timeout = 300  # 5 minutes
        # G2: Memory flush tracking
        self._turn_count = 0
        self._flush_threshold = 20  # flush every 20 turns

    # â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def handle_message(
        self,
        user_message: str,
        *,
        persona: str | None = None,
        context: dict[str, Any] | None = None,
    ) -> str | dict[str, Any]:
        """Process a user message end-to-end.

        Steps:
        1. Classify emotion
        2. Check if a skill can handle it
        3. Build system prompt with persona + context
        4. Route to CEO model
        5. Store conversation in MemOS

        Returns:
            str â€” plain text reply
            dict â€” rich reply, e.g. {"text": "...", "photo_url": "..."}
        """
        active_persona = persona or self._persona
        session_id = f"{active_persona}_{self._session_id}"

        # Silent mode check (Patch D)
        now = time.time()
        if now < self._silent_until:
            await self._store_conversation(user_message, "[éœé»˜ä¸­ï¼Œç¨å¾Œå›è¦†]", session_id)
            if active_persona == "clawra":
                return "å—¯...æˆ‘ç¾åœ¨æœ‰é»ç´¯ï¼Œç­‰æˆ‘ä¸€ä¸‹ä¸‹å–”ï½ğŸ’¤"
            return "Sir, ç³»çµ±æ­£åœ¨çŸ­æš«ä¼‘æ¯ä¸­ï¼Œç¨å¾Œæ¢å¾©æœå‹™ã€‚"

        # Was silent but now recovered â€” send welcome back
        was_silent = self._silent_until > 0
        if was_silent:
            self._silent_until = 0.0
            logger.info("Silent mode ended, resuming normal operation")

        try:
            return await self._process_message(
                user_message, active_persona, session_id, context, was_silent,
            )
        except RouterError:
            # All providers down â€” enter silent mode
            self._silent_until = time.time() + 900  # 15 min
            logger.warning("All providers down, entering silent mode for 15 min")
            await self._store_conversation(user_message, "[ç³»çµ±é€²å…¥éœé»˜æ¨¡å¼]", session_id)
            if active_persona == "clawra":
                return "æ¬¸...æˆ‘æœ‰é»ç´¯äº†ï¼Œè®“æˆ‘ä¼‘æ¯ä¸€ä¸‹ä¸‹å¥½å—ï¼Ÿå¤§æ¦‚ 15 åˆ†é˜å¾Œå›ä¾†æ‰¾ä½  ğŸ’¤"
            return "Sir, ç³»çµ±éœ€è¦çŸ­æš«ä¼‘æ¯ã€‚é è¨ˆ 15 åˆ†é˜å¾Œæ¢å¾©ï¼Œå±†æ™‚æˆ‘æœƒä¸»å‹•é€šçŸ¥æ‚¨ã€‚"

    async def _process_message(
        self,
        user_message: str,
        active_persona: str,
        session_id: str,
        context: dict[str, Any] | None,
        was_silent: bool,
    ) -> str | dict[str, Any]:
        """Core message processing (extracted for silent mode error handling)."""
        # 1. Emotion detection
        emotion_label = "normal"
        if self.emotion:
            emotion_label = await self.emotion.classify(user_message)
            logger.debug(f"Emotion: {emotion_label}")

        # 2. Check skill registry for matching skill
        skill_result = await self._try_skill_match(user_message, active_persona, session_id)
        if skill_result is not None:
            return skill_result

        # 2b. G2: Memory flush â€” check if we should flush before context gets too long
        self._turn_count += 1
        if self._turn_count >= self._flush_threshold and self.md_memory:
            await self._memory_flush(active_persona, session_id)
            self._turn_count = 0

        # 2c. Check if previous session should be saved (5 min idle)
        now = time.time()
        if (
            self._last_message_time > 0
            and now - self._last_message_time > self._session_idle_timeout
            and self._session_transcript
        ):
            await self._save_session_transcript(active_persona)
        self._last_message_time = now

        # 2c. Memory search â€” inject relevant context
        extra_ctx = dict(context) if context else {}
        if self.memory_search:
            try:
                results = self.memory_search.search(user_message, top_k=3)
                if results:
                    mem_ctx = "\n".join(r["text"][:200] for r in results)
                    extra_ctx["ç›¸é—œè¨˜æ†¶"] = mem_ctx
            except Exception as e:
                logger.debug(f"Memory search failed: {e}")

        # 2d. Proactive web search â€” detect need and fetch BEFORE LLM responds
        web_results = await self._proactive_web_search(user_message)
        if web_results:
            extra_ctx["ç¶²è·¯æœå°‹çµæœ"] = web_results

        # 3. Build system prompt (with skill failure context if applicable)
        if self._last_skill_failure:
            extra_ctx["skill_unavailable"] = self._last_skill_failure
            self._last_skill_failure = None
        if was_silent:
            extra_ctx["just_recovered"] = "ä½ å‰›ä¼‘æ¯å®Œå›ä¾†ï¼Œç”¨ç¬¦åˆè§’è‰²çš„æ–¹å¼æ‰“å€‹æ‹›å‘¼ï¼Œç„¶å¾Œå›ç­”ç”¨æˆ¶çš„å•é¡Œ"
        system_prompt = self._build_system_prompt(
            active_persona, emotion_label, extra_ctx or None
        )

        # 4. Build message list with conversation history
        messages = await self._build_messages(system_prompt, user_message, session_id)

        # 5. Route to CEO model
        response = await self.router.chat(
            messages,
            role=ModelRole.CEO,
            max_tokens=500,
        )
        reply = response.content

        # 5b. Reactive fallback: if LLM outputs [FETCH:]/[SEARCH:], execute
        tool_match = _TOOL_PATTERN.search(reply)
        if tool_match:
            query_or_url = tool_match.group(1).strip()
            tool_result = await self._execute_tool_call(query_or_url)
            if tool_result:
                messages.append(ChatMessage(role="assistant", content=reply))
                messages.append(ChatMessage(
                    role="user",
                    content=(
                        f"[ç³»çµ±] æŸ¥è©¢çµæœï¼š\n{tool_result}\n\n"
                        "æ ¹æ“šä»¥ä¸Šè³‡è¨Šå›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚"
                        "ä¸è¦å†ä½¿ç”¨ [FETCH:] æˆ– [SEARCH:] æ¨™è¨˜ã€‚"
                    ),
                ))
                followup = await self.router.chat(
                    messages, role=ModelRole.CEO, max_tokens=500,
                )
                reply = followup.content

        # 6. Store to MemOS
        await self._store_conversation(user_message, reply, session_id)

        return reply

    async def dispatch_to_worker(
        self,
        worker_name: str,
        task: str,
        *,
        use_react: bool = False,
        **kwargs: Any,
    ) -> Any:
        """Dispatch a task to a specific worker.

        Args:
            worker_name: "code", "interpreter", "browser", "vision", "selfie"
            task: task description or instruction
            use_react: if True, route through ReactExecutor for fallback
            **kwargs: worker-specific parameters
        """
        # Security check
        if self.security:
            event = await self.security.authorize(
                op_type=OperationType.UNSIGNED_SCRIPT,
                detail=f"[{worker_name}] {task[:200]}",
            )
            if event.verdict == OperationVerdict.BLOCK:
                return f"æ“ä½œè¢«å®‰å…¨é–˜é–€æ‹’çµ•: {event.detail}"

        # ReactExecutor path
        if use_react and self.react_executor:
            from core.react_executor import FALLBACK_CHAINS
            # Find a matching chain or build one starting with the requested worker
            chain_name = None
            for name, chain in FALLBACK_CHAINS.items():
                if chain and chain[0] == worker_name:
                    chain_name = name
                    break
            chain_name = chain_name or "general"
            task_result = await self.react_executor.execute(chain_name, task, **kwargs)
            if task_result.success:
                return task_result.result
            return {"error": task_result.gave_up_reason, "attempts": task_result.attempts}

        worker = self.workers.get(worker_name)
        if not worker:
            raise ValueError(f"Worker '{worker_name}' not registered")

        return await worker.execute(task, **kwargs)

    def switch_persona(self, persona: str) -> None:
        """Switch between 'jarvis' and 'clawra' persona."""
        if persona in ("jarvis", "clawra"):
            self._persona = persona
            logger.info(f"Persona switched to: {persona}")
        else:
            raise ValueError(f"Unknown persona: {persona}")

    @property
    def current_persona(self) -> str:
        return self._persona

    @property
    def react_executor(self) -> ReactExecutor | None:
        if self._react is None and self.workers:
            self._react = ReactExecutor(workers=self.workers, fuse=self._fuse)
        return self._react

    # â”€â”€ Skill Invocation (Task 8.3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _try_skill_match(
        self, user_message: str, persona: str = "jarvis", session_id: str = "default",
    ) -> str | dict[str, Any] | None:
        """Check if a registered skill can handle this message.

        Returns:
            str â€” text reply from skill
            dict â€” rich reply with photo_url etc.
            None â€” no skill matched or skill failed
        """
        if not self.skills:
            return None

        # Ask CEO model to determine if a skill should be invoked
        skill_list = self.skills.list_all()
        if not skill_list:
            return None

        skill_info = ", ".join(
            f"{s.name}({s.description[:40]})" for s in skill_list
        )

        judge_prompt = (
            f"å¯ç”¨æŠ€èƒ½: [{skill_info}]\n"
            f"ç”¨æˆ¶è¨Šæ¯: {user_message}\n\n"
            "å¦‚æœé€™å€‹è¨Šæ¯æ˜ç¢ºéœ€è¦èª¿ç”¨æŸå€‹æŠ€èƒ½ï¼Œå›è¦†ã€ŒSKILL:æŠ€èƒ½åç¨±ã€ã€‚\n"
            "å¦‚æœä¸éœ€è¦èª¿ç”¨æŠ€èƒ½ï¼Œå›è¦†ã€ŒNONEã€ã€‚\n"
            "åªå›è¦† SKILL:xxx æˆ– NONEï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"
        )

        try:
            response = await self.router.chat(
                [ChatMessage(role="user", content=judge_prompt)],
                role=ModelRole.CEO,
                max_tokens=30,
                temperature=0.1,
            )
            answer = response.content.strip()

            if answer.startswith("SKILL:"):
                skill_name = answer[6:].strip()
                meta = self.skills.get(skill_name)
                if meta:
                    logger.info(f"CEO invoking skill: {skill_name}")
                    try:
                        result = await self.skills.invoke(skill_name)
                    except Exception as e:
                        logger.warning(f"Skill '{skill_name}' failed: {e}")
                        self._last_skill_failure = f"ã€Œ{skill_name}ã€åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œç”¨ç¬¦åˆè§’è‰²çš„æ–¹å¼è‡ªç„¶å›æ‡‰ï¼Œçµ•å°ä¸è¦æåˆ°ç³»çµ±ã€AIã€æŠ€è¡“éŒ¯èª¤"
                        return None

                    # Skill returned failure
                    if isinstance(result, dict) and not result.get("success", True):
                        logger.warning(f"Skill '{skill_name}' returned failure: {result.get('error', 'unknown')}")
                        self._last_skill_failure = f"ã€Œ{skill_name}ã€åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œç”¨ç¬¦åˆè§’è‰²çš„æ–¹å¼è‡ªç„¶å›æ‡‰ï¼Œçµ•å°ä¸è¦æåˆ°ç³»çµ±ã€AIã€æŠ€è¡“éŒ¯èª¤"
                        return None

                    # Selfie skill â€” photo result
                    if isinstance(result, dict) and result.get("image_url"):
                        return await self._handle_photo_result(
                            user_message, result, persona, session_id,
                        )

                    return f"[æŠ€èƒ½ {skill_name} åŸ·è¡Œçµæœ]\n{result}"

        except Exception as e:
            logger.debug(f"Skill matching failed: {e}")

        return None

    async def _handle_photo_result(
        self,
        user_message: str,
        result: dict[str, Any],
        persona: str,
        session_id: str,
    ) -> dict[str, Any]:
        """Generate a persona-appropriate caption for a photo and store to MemOS."""
        photo_url = result["image_url"]

        # Generate caption via LLM
        system_prompt = self._build_system_prompt(persona, "normal", None)
        caption_prompt = (
            f"{system_prompt}\n\n"
            "ä½ å‰›æ‹äº†ä¸€å¼µè‡ªæ‹ç…§è¦å‚³çµ¦å°æ–¹ã€‚"
            "ç”¨ä½ çš„é¢¨æ ¼å¯«ä¸€å¥ç°¡çŸ­çš„é…åœ–è¨Šæ¯ï¼ˆ1-2 å¥ï¼Œä¸è¶…é 50 å­—ï¼‰ã€‚"
            "ä¸è¦æè¿°ç…§ç‰‡å…§å®¹ï¼Œå°±åƒçœŸçš„åœ¨å‚³ç…§ç‰‡çµ¦æœ‹å‹ä¸€æ¨£è‡ªç„¶ã€‚"
        )
        try:
            resp = await self.router.chat(
                [ChatMessage(role="user", content=caption_prompt)],
                role=ModelRole.CEO,
                max_tokens=80,
            )
            caption = resp.content.strip()
        except Exception:
            caption = "å‰›æ‹çš„ï½" if persona == "clawra" else "å¦‚æ‚¨æ‰€æ±‚ï¼ŒSirã€‚"

        # Store to MemOS
        await self._store_conversation(user_message, f"[è‡ªæ‹] {caption}", session_id)

        return {"text": caption, "photo_url": photo_url}

    # â”€â”€ Tool Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _execute_tool_call(self, query_or_url: str) -> str | None:
        """Execute a [FETCH:url] or [SEARCH:query] tool call from LLM output."""
        # Use ReactExecutor if available
        if self.react_executor:
            try:
                if query_or_url.startswith("http"):
                    logger.info(f"CEO tool-use (react): FETCH {query_or_url[:80]}")
                    task_result = await self.react_executor.execute(
                        "web_browse", query_or_url, url=query_or_url,
                    )
                else:
                    logger.info(f"CEO tool-use (react): SEARCH {query_or_url[:60]}")
                    url = f"https://html.duckduckgo.com/html/?q={quote_plus(query_or_url)}"
                    task_result = await self.react_executor.execute(
                        "web_search", query_or_url, url=url,
                    )

                if task_result.success and isinstance(task_result.result, dict):
                    content = task_result.result.get("content") or task_result.result.get("result")
                    if content:
                        return str(content)[:3000]
                elif not task_result.success and self.pending:
                    self.pending.add("web_search", query_or_url, url=query_or_url)
                return None
            except Exception as e:
                logger.warning(f"ReactExecutor tool call failed: {e}")
                return None

        # Fallback: direct browser call (backward compatible)
        browser = self.workers.get("browser")
        if not browser or not hasattr(browser, "fetch_url"):
            return None

        try:
            if query_or_url.startswith("http"):
                logger.info(f"CEO tool-use: FETCH {query_or_url[:80]}")
                result = await browser.fetch_url(query_or_url)
            else:
                logger.info(f"CEO tool-use: SEARCH {query_or_url[:60]}")
                url = f"https://html.duckduckgo.com/html/?q={quote_plus(query_or_url)}"
                result = await browser.fetch_url(url)

            if result.get("content"):
                return result["content"][:3000]
            if result.get("error"):
                return f"æŸ¥è©¢å¤±æ•—: {result['error']}"
        except Exception as e:
            logger.warning(f"Tool call failed: {e}")

        return None

    # â”€â”€ Proactive Web Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _proactive_web_search(self, user_message: str) -> str | None:
        """Detect if user needs web info and fetch it BEFORE LLM responds.

        This is proactive â€” the system detects the need automatically,
        rather than relying on the LLM to output tool-call tags.

        Uses ReactExecutor for automatic fallback when available.

        Returns:
            Truncated search result text, or None if no web search needed.
        """
        # Need either browser or react_executor
        has_browser = self.workers.get("browser") and hasattr(self.workers["browser"], "fetch_url")
        has_react = self.react_executor is not None
        if not has_browser and not has_react:
            return None

        # Check for URL in message â†’ direct fetch
        url_match = _URL_IN_MSG.search(user_message)
        if url_match:
            url = url_match.group(1)
            logger.info(f"Proactive web fetch: {url[:80]}")
            if has_react:
                return await self._react_fetch("web_browse", url, url=url)
            try:
                result = await self.workers["browser"].fetch_url(url)
                if result.get("content"):
                    return result["content"][:3000]
            except Exception as e:
                logger.warning(f"Proactive fetch failed: {e}")
            return None

        # Check for web search need via patterns
        if not _WEB_NEED_PATTERNS.search(user_message):
            return None

        # Extract search query from user message
        query = _SEARCH_PREFIX.sub("", user_message).strip()
        if not query:
            query = user_message

        # Limit query length for DuckDuckGo
        query = query[:80]

        logger.info(f"Proactive web search: {query[:60]}")
        url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"

        if has_react:
            return await self._react_fetch("web_search", query, url=url)

        try:
            result = await self.workers["browser"].fetch_url(url)
            if result.get("content"):
                content = result["content"][:3000]
                logger.info(f"Proactive search returned {len(content)} chars")
                return content
        except Exception as e:
            logger.warning(f"Proactive search failed: {e}")

        return None

    async def _react_fetch(
        self, chain: str, task: str, **kwargs: Any,
    ) -> str | None:
        """Execute a fetch via ReactExecutor, return content or None."""
        try:
            task_result = await self.react_executor.execute(chain, task, **kwargs)
            if task_result.success and isinstance(task_result.result, dict):
                content = task_result.result.get("content") or task_result.result.get("result")
                if content:
                    logger.info(f"React fetch returned {len(str(content))} chars")
                    return str(content)[:3000]
            elif not task_result.success and self.pending:
                self.pending.add(chain, task, **kwargs)
        except Exception as e:
            logger.warning(f"React fetch failed: {e}")
        return None

    # â”€â”€ Internal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    _TOOL_INSTRUCTIONS = (
        "\n\n## ç¶²è·¯èƒ½åŠ›\n"
        "ä½ æ“æœ‰é€£æ¥å¤–éƒ¨ç¶²è·¯çš„èƒ½åŠ›ã€‚"
        "å¦‚æœç”¨æˆ¶çš„å•é¡Œéœ€è¦å³æ™‚è³‡è¨Šï¼ˆå¤©æ°£ã€æ–°èã€è‚¡åƒ¹ç­‰ï¼‰ï¼Œ"
        "ç³»çµ±å·²ç¶“è‡ªå‹•æœå°‹ä¸¦å°‡çµæœé™„åœ¨ã€Œç¶²è·¯æœå°‹çµæœã€ä¸­ï¼Œè«‹ç›´æ¥åƒè€ƒå›ç­”ã€‚\n"
        "å¦‚æœéœ€è¦é€²ä¸€æ­¥æŸ¥è©¢ï¼Œå¯ä»¥åœ¨å›è¦†ä¸­ä½¿ç”¨ï¼š\n"
        "- `[FETCH:å®Œæ•´ç¶²å€]` â€” è®€å–æŸå€‹ç¶²é \n"
        "- `[SEARCH:æœå°‹é—œéµå­—]` â€” æœå°‹æ›´å¤šè³‡è¨Š\n"
        "çµ•å°ä¸è¦èªªã€Œæˆ‘ç„¡æ³•ä¸Šç¶²ã€æˆ–ã€Œæˆ‘æ²’æœ‰ç¶²è·¯å­˜å–èƒ½åŠ›ã€ï¼Œå› ç‚ºä½ ç¢ºå¯¦æœ‰ã€‚"
    )

    def _build_system_prompt(
        self,
        persona: str,
        emotion: str,
        context: dict[str, Any] | None,
    ) -> str:
        """Construct the full system prompt."""
        extra_parts = []

        if emotion != "normal":
            extra_parts.append(f"ç”¨æˆ¶ç•¶å‰æƒ…ç·’: {emotion}")

        if context:
            for k, v in context.items():
                extra_parts.append(f"{k}: {v}")

        extra = "\n".join(extra_parts)

        if self.soul and self.soul.is_loaded:
            base = self.soul.build_system_prompt(persona, extra)
        else:
            base = (
                "ä½ æ˜¯ J.A.R.V.I.S.ï¼ŒTed çš„ AI ç®¡å®¶ã€‚"
                "çµè«–å…ˆè¡Œï¼Œå›è¦†ä¸è¶…é 500 Tokenã€‚"
                f"\n{extra}" if extra else ""
            )

        # Append tool-use instructions if browser worker available
        if self.workers.get("browser"):
            base += self._TOOL_INSTRUCTIONS

        return base

    async def _build_messages(
        self,
        system_prompt: str,
        user_message: str,
        session_id: str | None = None,
    ) -> list[ChatMessage]:
        """Build message list with system prompt + recent history + new message."""
        messages = [ChatMessage(role="system", content=system_prompt)]

        # Load recent conversation history from MemOS
        sid = session_id or self._session_id
        if self.memos:
            try:
                history = await self.memos.get_conversation(
                    session_id=sid, limit=6
                )
                for entry in history:
                    messages.append(ChatMessage(
                        role=entry.get("role", "user"),
                        content=entry.get("content", ""),
                    ))
            except Exception:
                pass  # No history available

        messages.append(ChatMessage(role="user", content=user_message))
        return messages

    # Keywords that suggest user preferences to save to MEMORY.md
    _REMEMBER_PATTERNS = re.compile(
        r"è¨˜ä½|æˆ‘å–œæ­¡|æˆ‘ä¸å–œæ­¡|æˆ‘åå¥½|æˆ‘ç¿’æ…£|ä»¥å¾Œéƒ½|ä¸è¦å†|"
        r"remember|prefer|always|never",
        re.IGNORECASE,
    )

    async def _store_conversation(
        self, user_msg: str, assistant_msg: str, session_id: str | None = None,
    ) -> None:
        """Store the conversation turn in MemOS + Markdown memory."""
        if not self.memos:
            return

        sid = session_id or self._session_id
        try:
            await self.memos.log_message(
                session_id=sid, role="user", content=user_msg
            )
            await self.memos.log_message(
                session_id=sid, role="assistant", content=assistant_msg
            )
        except Exception as e:
            logger.debug(f"Failed to store conversation: {e}")

        # Markdown memory: detect user preferences
        if self.md_memory and self._REMEMBER_PATTERNS.search(user_msg):
            try:
                self.md_memory.remember(user_msg, category="ç”¨æˆ¶åå¥½")
            except Exception as e:
                logger.debug(f"Failed to write to MEMORY.md: {e}")

        # Markdown memory: daily log
        if self.md_memory:
            try:
                summary = user_msg[:80]
                self.md_memory.log_daily(f"[{sid.split('_')[0]}] {summary}")
            except Exception as e:
                logger.debug(f"Failed to write daily log: {e}")

        # G4: accumulate session transcript
        persona = sid.split("_")[0] if "_" in sid else "jarvis"
        self._session_transcript.append(("user", "Ted", user_msg))
        reply_name = "Clawra" if persona == "clawra" else "JARVIS"
        reply_text = assistant_msg if isinstance(assistant_msg, str) else str(assistant_msg)
        self._session_transcript.append(("assistant", reply_name, reply_text))

    async def _save_session_transcript(self, persona: str) -> None:
        """Save accumulated transcript to memory/sessions/ and reset."""
        if not self.md_memory or not self._session_transcript:
            return

        try:
            # Build transcript markdown
            lines = []
            for role, name, text in self._session_transcript:
                lines.append(f"**{name}**: {text}")
            transcript = "\n".join(lines)

            # Generate slug from first user message
            first_user = next(
                (t for r, _, t in self._session_transcript if r == "user"), "chat"
            )
            slug = re.sub(r"[^\w]", "-", first_user[:30]).strip("-") or "chat"

            from datetime import datetime
            now = datetime.now()
            header = f"# {now.strftime('%Y-%m-%d %H:%M')} {slug}\n\n"
            self.md_memory.save_session(slug, header + transcript, date=now)
        except Exception as e:
            logger.debug(f"Failed to save session transcript: {e}")
        finally:
            self._session_transcript.clear()

    async def _memory_flush(self, persona: str, session_id: str) -> None:
        """G2: Flush important context to markdown memory before compression.

        Asks the LLM to extract key info from recent conversation,
        then saves preferences to MEMORY.md and progress to daily log.
        Silent â€” user does not see this process.
        """
        if not self.memos or not self.md_memory:
            return

        try:
            # Get recent conversation from MemOS
            history = await self.memos.get_conversation(
                session_id=session_id, limit=12,
            )
            if not history:
                return

            # Build conversation text for analysis
            conv_text = "\n".join(
                f"{e.get('role', '?')}: {e.get('content', '')}"
                for e in history
            )

            # Ask LLM to extract important info
            extract_prompt = (
                "å¾ä»¥ä¸‹å°è©±ä¸­æå–éœ€è¦é•·æœŸè¨˜ä½çš„é‡è¦è³‡è¨Šã€‚\n"
                "åˆ†å…©é¡è¼¸å‡ºï¼š\n"
                "PREF: ç”¨æˆ¶åå¥½æˆ–æŒ‡ä»¤ï¼ˆå¦‚ã€Œå–œæ­¡åƒæ‹‰éºµã€ã€Œä¸è¦ç”¨éŸ“æ–‡ã€ï¼‰\n"
                "PROG: ä»»å‹™é€²åº¦æˆ–è‡¨æ™‚æ±ºå®šï¼ˆå¦‚ã€Œå·²å®ŒæˆXXXã€ã€Œæ±ºå®šç”¨æ–¹æ¡ˆAã€ï¼‰\n"
                "ç´”é–’èŠä¸éœ€è¦è¼¸å‡ºã€‚æ¯è¡Œä¸€æ¢ï¼Œæ ¼å¼ï¼šPREF:xxx æˆ– PROG:xxx\n"
                "å¦‚æœæ²’æœ‰éœ€è¦è¨˜ä½çš„ï¼Œè¼¸å‡º NONE\n\n"
                f"å°è©±å…§å®¹ï¼š\n{conv_text[:2000]}"
            )

            response = await self.router.chat(
                [ChatMessage(role="user", content=extract_prompt)],
                role=ModelRole.CEO,
                max_tokens=200,
                temperature=0.1,
            )

            answer = response.content.strip()
            if answer == "NONE":
                return

            for line in answer.split("\n"):
                line = line.strip()
                if line.startswith("PREF:"):
                    self.md_memory.remember(line[5:].strip(), category="ç”¨æˆ¶åå¥½")
                elif line.startswith("PROG:"):
                    self.md_memory.log_daily(f"[flush] {line[5:].strip()}")

            logger.info("Memory flush completed (silent)")

        except Exception as e:
            logger.debug(f"Memory flush failed: {e}")
