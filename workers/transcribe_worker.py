"""Transcribe Worker â€” long audio ASR + two-stage LLM summary.

For meeting recordings and long audio files (not short voice clips).
Uses æ™ºè­œ GLM-ASR for transcription, then ModelRouter CEO chain for
a structured two-stage summary.

Usage:
    worker = TranscribeWorker(model_router=router, zhipu_key="...")
    result = await worker.process_audio("meeting.m4a", context="é€±æœƒ")
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Any

from loguru import logger


SUPPORTED_FORMATS = {".ogg", ".mp3", ".m4a", ".wav", ".flac", ".opus"}
CHUNK_SIZE = 3000  # chars per summary segment


class TranscribeWorker:
    """Long-form audio transcription + structured meeting summary."""

    def __init__(
        self,
        model_router: Any = None,
        zhipu_key: str = "",
    ) -> None:
        self.name = "transcribe"
        self.router = model_router
        self.zhipu_key = zhipu_key

    # â”€â”€ Worker interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def execute(self, task: str, **kwargs: Any) -> dict[str, Any]:
        """Worker interface.

        Args:
            task: Path to audio file.
            **kwargs: context (str) â€” topic hint for better summary.

        Returns:
            dict with result (summary), transcript, source, worker.
        """
        context = kwargs.get("context", "")
        return await self.process_audio(task, context=context)

    # â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def process_audio(
        self, audio_path: str, context: str = "",
    ) -> dict[str, Any]:
        """Transcribe audio and produce a structured summary.

        Args:
            audio_path: Path to the audio file.
            context: Optional topic/meeting context for better summary.

        Returns:
            dict with keys: result (summary str), transcript (raw text),
            source, worker.  On error: error key present.
        """
        path = Path(audio_path)
        if not path.exists():
            return {"error": f"æª”æ¡ˆä¸å­˜åœ¨: {audio_path}", "worker": self.name}

        if path.suffix.lower() not in SUPPORTED_FORMATS:
            return {
                "error": f"ä¸æ”¯æ´çš„æ ¼å¼: {path.suffix} (æ”¯æ´: {', '.join(sorted(SUPPORTED_FORMATS))})",
                "worker": self.name,
            }

        if not self.zhipu_key:
            return {"error": "æœªè¨­å®š ZHIPU_API_KEYï¼Œç„¡æ³•è½‰éŒ„", "worker": self.name}

        # Step 1: ASR transcription
        transcript = await self._transcribe(path)
        if not transcript:
            return {"error": "è½‰éŒ„å¤±æ•—ï¼Œæœªå–å¾—æ–‡å­—", "worker": self.name}

        # Step 2: Two-stage summary
        summary = await self._two_stage_summary(transcript, context)

        return {
            "result": summary,
            "transcript": transcript,
            "source": "transcribe",
            "worker": self.name,
        }

    # â”€â”€ ASR via zhipuai SDK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _transcribe(self, audio_path: Path) -> str:
        """Transcribe audio using æ™ºè­œ GLM-ASR.

        Uses the zhipuai SDK synchronously in a thread pool to avoid
        blocking the event loop.
        """
        try:
            transcript = await asyncio.to_thread(
                self._transcribe_sync, audio_path,
            )
            logger.info(
                f"TranscribeWorker: transcribed {audio_path.name} "
                f"({len(transcript)} chars)"
            )
            return transcript
        except Exception as e:
            logger.error(f"TranscribeWorker ASR failed: {e}")
            return ""

    def _transcribe_sync(self, audio_path: Path) -> str:
        """Synchronous transcription call (run in thread)."""
        from zhipuai import ZhipuAI

        client = ZhipuAI(api_key=self.zhipu_key)
        with open(audio_path, "rb") as f:
            result = client.audio.transcriptions.create(
                model="glm-asr-2512",
                file=f,
            )
        # result.text contains the transcribed text
        return result.text if hasattr(result, "text") else str(result)

    # â”€â”€ Two-stage LLM summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _two_stage_summary(self, transcript: str, context: str) -> str:
        """Produce a structured summary via two LLM passes.

        Stage 1: Summarize each chunk individually.
        Stage 2: Merge chunk summaries into a final structured output.
        """
        if not self.router:
            return transcript[:2000]  # fallback: raw truncated text

        chunks = self._split_transcript(transcript)

        # Stage 1: per-chunk summaries
        chunk_summaries: list[str] = []
        for i, chunk in enumerate(chunks):
            prompt = (
                f"é€™æ˜¯ä¸€æ®µéŒ„éŸ³çš„ç¬¬ {i + 1}/{len(chunks)} éƒ¨åˆ†é€å­—ç¨¿ã€‚"
                f"{f'ä¸»é¡Œ: {context}ã€‚' if context else ''}\n\n"
                f"{chunk}\n\n"
                f"è«‹ç”¨ç¹é«”ä¸­æ–‡æ‘˜è¦é€™æ®µå…§å®¹çš„é‡é»žï¼ˆ3-5 å¥ï¼‰ï¼Œ"
                f"ä¿ç•™é—œéµæ•¸å­—ã€äººåã€æ±ºè­°ã€‚"
            )
            summary = await self._llm_generate(prompt, max_tokens=400)
            chunk_summaries.append(summary)

        # Stage 2: merge into structured summary
        merged = "\n\n".join(
            f"ã€ç¬¬ {i + 1} æ®µã€‘\n{s}" for i, s in enumerate(chunk_summaries)
        )

        final_prompt = (
            f"ä»¥ä¸‹æ˜¯ä¸€æ®µ{'æœƒè­°' if context else ''}éŒ„éŸ³çš„åˆ†æ®µæ‘˜è¦ã€‚"
            f"{f'ä¸»é¡Œ: {context}ã€‚' if context else ''}\n\n"
            f"{merged}\n\n"
            f"è«‹æ•´åˆæˆä¸€ä»½çµæ§‹åŒ–æ‘˜è¦ï¼ŒåŒ…å«ï¼š\n"
            f"ðŸ“‹ æœƒè­°æ‘˜è¦ï¼ˆ3-5 å¥ç¸½çµï¼‰\n"
            f"âœ… é—œéµæ±ºè­°ï¼ˆæ¢åˆ—ï¼‰\n"
            f"ðŸ“Œ è¡Œå‹•é …ç›®ï¼ˆèª°ã€åšä»€éº¼ã€ä½•æ™‚ï¼‰\n"
            f"â“ æœªè§£æ±ºå•é¡Œï¼ˆå¦‚æœ‰ï¼‰\n"
            f"ðŸ”¢ é‡è¦æ•¸å­—ï¼ˆå¦‚æœ‰ï¼‰\n\n"
            f"ç”¨ç¹é«”ä¸­æ–‡ï¼Œçµè«–å…ˆè¡Œã€‚"
        )

        return await self._llm_generate(final_prompt, max_tokens=800)

    def _split_transcript(self, text: str) -> list[str]:
        """Split transcript into chunks of ~CHUNK_SIZE chars."""
        if len(text) <= CHUNK_SIZE:
            return [text]

        chunks: list[str] = []
        start = 0
        while start < len(text):
            end = start + CHUNK_SIZE
            # Try to break at a sentence boundary
            if end < len(text):
                for sep in ("ã€‚", ".", "\n", "ï¼Œ", ","):
                    pos = text.rfind(sep, start, end)
                    if pos > start:
                        end = pos + 1
                        break
            # Safety: ensure forward progress
            if end <= start:
                end = start + CHUNK_SIZE
            chunks.append(text[start:end])
            start = end
        return chunks

    # â”€â”€ LLM helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _llm_generate(self, prompt: str, max_tokens: int = 500) -> str:
        """Generate text via ModelRouter CEO chain."""
        if not self.router:
            return ""
        try:
            from clients.base_client import ChatMessage
            from core.model_router import ModelRole

            response = await self.router.chat(
                [ChatMessage(role="user", content=prompt)],
                role=ModelRole.CEO,
                max_tokens=max_tokens,
            )
            return response.content
        except Exception as e:
            logger.warning(f"TranscribeWorker LLM failed: {e}")
            return ""

    async def close(self) -> None:
        """Cleanup (no persistent resources)."""
